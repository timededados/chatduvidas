// api/chat.js
import OpenAI from "openai";
import fs from "fs/promises";
import path from "path";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

const DATA_DIR = path.join(process.cwd(), "data");
const BOOK_PATH = path.join(DATA_DIR, "abramede_texto.json");
const EMB_PATH = path.join(DATA_DIR, "abramede_embeddings.json");

const EMB_MODEL = "text-embedding-3-small";
const CHAT_MODEL = "gpt-4o-mini";
const TOP_K = 6;
const MAX_CONTEXT_TOKENS = 3000;

// ------------------ Fun√ß√µes auxiliares ------------------
function dot(a, b) {
  return a.reduce((s, v, i) => s + v * b[i], 0);
}
function norm(a) {
  return Math.sqrt(a.reduce((s, x) => s + x * x, 0));
}
function cosineSim(a, b) {
  return dot(a, b) / (norm(a) * norm(b) + 1e-8);
}

// ------------------ Fun√ß√£o principal ------------------
export default async function handler(req, res) {
  if (req.method !== "POST") return res.status(405).end();

  try {
    const { question } = req.body;
    if (!question || !question.trim())
      return res.status(400).json({ error: "Pergunta vazia" });

    // 1Ô∏è‚É£ Gerar varia√ß√µes da pergunta
    const variationPrompt = `
Voc√™ √© um assistente que ajuda a gerar varia√ß√µes de consulta de busca em textos t√©cnicos.
Dada a pergunta do usu√°rio, gere at√© 6 varia√ß√µes curtas (1-12 palavras cada) que mantenham o sentido,
sem adicionar conte√∫do novo. Responda apenas em JSON: {"variations": ["..."]}

Pergunta: """${question}"""
`;

    const varResp = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages: [
        { role: "system", content: "Voc√™ gera varia√ß√µes de consultas para busca textual sem adicionar conte√∫do." },
        { role: "user", content: variationPrompt }
      ],
      temperature: 0.2,
      max_tokens: 300
    });

    const rawVar = varResp.choices?.[0]?.message?.content?.trim() || "";
    let variations = [question];
    try {
      const parsed = JSON.parse(rawVar);
      if (parsed?.variations?.length) variations = parsed.variations;
    } catch {
      variations = rawVar.split(/\r?\n/).filter(Boolean).slice(0, 6);
      if (!variations.length) variations = [question];
    }

    // 2Ô∏è‚É£ Carrega o livro
    const bookRaw = await fs.readFile(BOOK_PATH, "utf8");
    const pages = JSON.parse(bookRaw);
    const pageMap = new Map(pages.map(p => [p.pagina, p.texto]));

    // 3Ô∏è‚É£ Carrega ou gera embeddings
    let pageEmbeddings;
    try {
      const embRaw = await fs.readFile(EMB_PATH, "utf8");
      pageEmbeddings = JSON.parse(embRaw);
    } catch {
      pageEmbeddings = [];
      for (const p of pages) {
        const txt = (p.texto || "").slice(0, 2000);
        const emb = await openai.embeddings.create({
          model: EMB_MODEL,
          input: txt || " "
        });
        pageEmbeddings.push({ pagina: p.pagina, embedding: emb.data[0].embedding });
      }
      await fs.writeFile(EMB_PATH, JSON.stringify(pageEmbeddings), "utf8");
    }

    // 4Ô∏è‚É£ Busca sem√¢ntica
    const aggregate = new Map();
    for (const query of variations) {
      const qEmb = await openai.embeddings.create({
        model: EMB_MODEL,
        input: query
      });
      const queryEmb = qEmb.data[0].embedding;
      for (const pe of pageEmbeddings) {
        const score = cosineSim(queryEmb, pe.embedding);
        const prev = aggregate.get(pe.pagina) || 0;
        aggregate.set(pe.pagina, prev + score);
      }
    }

    // Ordena p√°ginas mais relevantes
    const sorted = Array.from(aggregate.entries())
      .map(([pagina, score]) => ({ pagina, score }))
      .sort((a, b) => b.score - a.score)
      .slice(0, TOP_K);

    // üîπ Inclui 1 p√°gina anterior e 1 posterior para cada selecionada
    const expandedSet = new Set();
    for (const { pagina } of sorted) {
      expandedSet.add(pagina);
      if (pagina > 1) expandedSet.add(pagina - 1);
      expandedSet.add(pagina + 1);
    }

    // Elimina duplicadas e ordena
    const expandedPages = Array.from(expandedSet).sort((a, b) => a - b);

    const selected = expandedPages.map(pagina => ({
      pagina,
      texto: pageMap.get(pagina) || ""
    }));

    // 5Ô∏è‚É£ Monta o contexto
    let contextBuilder = [];
    let totalLen = 0;
    for (const s of selected) {
      const snippet = (s.texto || "").trim();
      const len = snippet.length;
      if (totalLen + len > MAX_CONTEXT_TOKENS * 4) break;
      contextBuilder.push(`--- P√°gina ${s.pagina} ---\n${snippet}\n`);
      totalLen += len;
    }

    const contextText = contextBuilder.join("\n");
    if (!contextText.trim())
      return res.json({ answer: "N√£o encontrei conte√∫do no livro." });

    // 6Ô∏è‚É£ Prompt para resposta literal e restrita
    const systemInstruction = `
Voc√™ √© um assistente que responde perguntas exclusivamente com base no texto abaixo.
‚ö†Ô∏è REGRAS IMPORTANTES:
- N√ÉO adicione informa√ß√µes externas ao texto.
- N√ÉO use conhecimento m√©dico, t√©cnico ou enciclop√©dico de fora do livro.
- S√ì utilize frases, trechos ou par√°frases curtas do texto fornecido.
- N√ÉO preencha lacunas nem interprete significados.
- Se o texto n√£o contiver a resposta, diga exatamente:
  "N√£o encontrei conte√∫do no livro."
- Cite sempre a(s) p√°gina(s) usada(s) entre par√™nteses, ex: (p. 45).
`;

    const userPrompt = `
Conte√∫do do livro (trechos das p√°ginas selecionadas):

${contextText}

Pergunta do usu√°rio:
"""${question}"""

Responda de forma literal, usando apenas o texto acima.
`;

    const chatResp = await openai.chat.completions.create({
      model: CHAT_MODEL,
      messages: [
        { role: "system", content: systemInstruction },
        { role: "user", content: userPrompt }
      ],
      temperature: 0,
      max_tokens: 800
    });

    const answer =
      chatResp.choices?.[0]?.message?.content?.trim() ||
      "N√£o encontrei conte√∫do no livro.";

    return res.status(200).json({
      answer,
      used_pages: expandedPages
    });

  } catch (err) {
    console.error("Erro no /api/chat:", err);
    return res.status(500).json({ error: String(err) });
  }
}
